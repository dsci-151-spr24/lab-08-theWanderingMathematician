---
title: "Lab 8 - Grading the professor, Pt. 1"
subtitle: "Modelling with a single predictor"
author: "Micaiah Balonek"
date: "20 March, 2024"
output: html_document
link-citations: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, warning = FALSE, message = FALSE, error = FALSE)
```

Many college courses conclude by giving students the opportunity to evaluate the course and the instructor anonymously.
However, the use of these student evaluations as an indicator of course quality and teaching effectiveness is often criticized because these measures may reflect the influence of non-teaching related characteristics, such as the physical appearance of the instructor.
The article titled, "Beauty in the classroom: instructors' pulchritude and putative pedagogical productivity" (Hamermesh and Parker, 2005) found that instructors who are viewed to be better looking receive higher instructional ratings.
(Daniel S. Hamermesh, Amy Parker, Beauty in the classroom: instructors pulchritude and putative pedagogical productivity, Economics of Education Review, Volume 24, Issue 4, August 2005, Pages 369-376, ISSN 0272-7757, 10.1016/j.econedurev.2004.07.013. <http://www.sciencedirect.com/science/article/pii/S0272775704001165>.)

In this lab you will analyze the data from this study in order to learn what goes into a positive professor evaluation.

The data were gathered from end of semester student evaluations for a large sample of professors from the University of Texas at Austin.
In addition, six students rated the professors' physical appearance.
(This is a slightly modified version of the original data set that was released as part of the replication data for Data Analysis Using Regression and Multilevel/Hierarchical Models (Gelman and Hill, 2007).) The result is a data frame where each row contains a different course and columns represent variables about the courses and professors.

# Learning goals

-   Fitting a linear regression with a single numerical and categorical predictor
-   Interpreting regression output in context of the data
-   Comparing models

# Getting started

Go to the course GitHub organization and locate your homework repo, clone it in RStudio and open the R Markdown document.
Knit the document to make sure it compiles without errors.

## Warm up

Let's warm up with some simple exercises.
Update the YAML of your R Markdown file with your information, knit, commit, and push your changes.
Make sure to commit with a meaningful commit message.
Then, go to your repo on GitHub and confirm that your changes are visible in your Rmd **and** md files.
If anything is missing, commit and push again.

## Packages

We'll use the **tidyverse** package for much of the data wrangling and visualisation, the **tidymodels** package for modeling and inference, and the data lives in the **dsbox** package.
These packages are already installed for you.
You can load them by running the following in your Console:

```{r load-packages, eval = TRUE, message = FALSE}
library(tidyverse) 
library(tidymodels)
library(openintro)
library(flextable)
```

## Data

The data can be found in the **openintro** package, and it's called `evals`.
Since the dataset is distributed with the package, we don't need to load it separately; it becomes available to us when we load the package.
You can find out more about the dataset by inspecting its documentation, which you can access by running `?evals` in the Console or using the Help menu in RStudio to search for `evals`.
You can also find this information [here](https://www.openintro.org/data/index.php?data=evals).

# Exercises

## Exploratory Data Analysis

1.  Visualize the distribution of `score`.
    Is the distribution skewed?
    What does that tell you about how students rate courses?
    Is this what you expected to see?
    Why, or why not?
    Include any summary statistics and visualizations you use in your response.
    
```{r score-distribution, eval=TRUE}

ggplot(evals, aes(x = score)) +
  geom_histogram(binwidth = 0.2) +
  xlim(0, 5) +
  theme_minimal() +
  labs(title = "Distrubution of scores in student course evaluations", x = "class score", y = "")

evals %>%
  summarise(mean = mean(score), median = median(score), min = min(score), max = max(score)) %>% 
  flextable() %>% autofit()
```

- *We can see from both the histogram and the summary statisctics that there are no evaluations below 2, and that from there until 5, the amount of values is positively correlated to the score, meaning that there are many more scores with higher scores than lower scores. However, there are several possible reasons for this: it could be because, on average, the classes were all quite well-taught, or maybe the students don't want to give overly negative ratings to their professors. We don't have enough data to tell yet between these two or other possibilities.*

2.  Visualize and describe the relationship between `score` and `bty_avg`.

```{margin-figure}
**Hint:** See the help page for the function at http://ggplot2.tidyverse.org/reference/index.html.
```

```{r score-beauty-scatterplot, eval = TRUE}
ggplot(evals, aes(y = score, x = bty_avg)) +
  geom_point() +
  theme_minimal() +
  geom_smooth(se = FALSE) +
  labs(title = "Class evaluations", subtitle = "And their relation to the beauty rating of the course professor", y = "Evaluation score", x = "Average beauty rating")
```

- *The relation between a professor's evaluation scores and beauty rating is seen to be slightly positive in this graph, with few points found on the bottom-right, but many found on the upper-left region of the graph, but also many others both above and below the regression line on the left half of the graph, presumably canceling eachother out, and a lot of extra points in the top part.*

3.  Recreate the scatterplot from Exercise 2, but this time use\
    `geom_jitter()`? What does "jitter" mean? What was misleading about the initial scatterplot?

```{r score-beauty-jitterplot, eval = TRUE}
ggplot(evals, aes(y = score, x = bty_avg)) +
  geom_smooth(se = FALSE, alpha = 0.5) +
  geom_jitter(alpha = 0.3) +
  theme_minimal() +
  labs(title = "Class evaluations", subtitle = "And their relation to the beauty rating of the course professor", y = "Evaluation score", x = "Average beauty rating")
```

- *Using `geom_jitter` adds some random noise to each point. Doing this will make it visible where there are multiple points in the same area, while they cover eachother in the original scatterplot, so that the 'jitterplot' more accurately shows point density (even moreso with the low alpha value I have selected). The jitterplot shows  that there are actually lots of points in the upper, mid-left side, as well as a large population of points with x-values between 4 and 5. Generally, it seems that there are a larger amount of points above the curve, but the points below it seem to be much more distant than the points above it.*

üß∂ ‚úÖ ‚¨ÜÔ∏è *If you haven't done so recently, knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.*

## Linear regression with a numerical predictor

```{margin-figure}
Linear model is in the form $\hat{y} = b_0 + b_1 x$.
```

4.  Let's see if the apparent trend in the plot is something more than natural variation.
    Fit a linear model called `score_bty_fit` to predict average professor evaluation `score` by average beauty rating (`bty_avg`).
    Based on the regression output, write the linear model.

```{r score-bty-regression}
score_bty_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score~bty_avg, data = evals) %>%
  tidy()

score_bty_fit %>%
  flextable() %>% autofit()
```

- ***The linear model is: 

$$ \widehat{score}_i = 4.28 - 0.13 \times btyavg_i $$

- *This is a quite low correlation, even lower than it originally appeared on the graph*

5.  Recreate the scatterplot from Exercise 2, and add the regression line to this plot in orange colour, with shading for the uncertainty of the line turned off.

```{r bty-regression-plot, eval = TRUE}
ggplot(evals, aes(y = score, x = bty_avg)) +
  geom_point() +
  theme_minimal() +
  geom_smooth(colour = "orange", method = "lm", se = FALSE) +
  labs(title = "Class evaluations", subtitle = "And their relation to the beauty rating of the course professor", y = "Evaluation score", x = "Average beauty rating")
```

6.  Interpret the slope of the linear model in context of the data.

- *On average, for every 1-point increase in the professor's Average Beauty Rating, the class' Evaluation Score goes up by 0.067 points.*

7.  Interpret the intercept of the linear model in context of the data.
    Comment on whether or not the intercept makes sense in this context.

- *According to the linear model, If a professor's Average Beauty Rating were 0, their class score would be, on average, 3.88. However, this is just an extrapolation of the data, not something actually shown in it, since the lowest value of Average Beauty Rating in the data is only `r min(evals$bty_avg)` points, so the intercept value doesn't really make sense.*

üß∂ ‚úÖ ‚¨ÜÔ∏è *If you haven't done so recently, knit, commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.*

## Linear regression with a categorical predictor

9.  Fit a new linear model called `score_gender_fit` to predict average professor evaluation `score` based on `gender` of the professor.
    Based on the regression output, write the linear model and interpret the slope and intercept in context of the data.

```{r gender-score-regression}
score_gender_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score~gender, data = evals) %>%
  tidy()

score_gender_fit %>%
  flextable() %>% autofit()
```

- ***The linear model is: 

$$ \widehat{score}_i = 4.09 + 0.14 \times male_i$$

- *This means that the y-intercept represents the average evaluation score for classes with female professors (when `male = 0`), and the slope represents the difference between this value and the average value for male professors (when `male = 1).*

10. What is the equation of the line corresponding to male professors?
    What is it for female professors?

- ***In this model, the equation for the line corresponding to male professors is $score = 4.23$, and the same for female professors is $score = 4.09$.***

11. Fit a new linear model called `score_rank_fit` to predict average professor evaluation `score` based on `rank` of the professor.
    Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data.

```{r rank-score-regression}
score_rank_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score~rank, data = evals) %>%
  tidy()

score_rank_fit %>%
  flextable() %>% autofit()
```

- ***This linear model is:***

$$ \widehat{score}_i = 4.28 - 0.13 \times tenureTrack_i - 0.15 \times tenured_i$$

- *The intercept (4.28) represents the average evaluation score of a teaching professor, while the two slopes (-0.13 and -0.15) represent the differences in average scores between each of the other two ranks and the "teaching" rank, so that the equation for the 'teaching' rank is `score = 4.28`, while the equation for the 'tenure track' rank is $ \widehat{score} = 4.28 - 0.13 = 4.15$ and that for the 'tenured' rank is $ \widehat{score} = 4.28 - 0.15 = 4.13$.*

12. Create a new variable called `rank_relevel` where `"tenure track"` is the baseline level.

```{r rank-releveling}
evals <- evals %>%
  mutate(rank_relevel = relevel(rank, ref = "tenure track"))
```

13. Fit a new linear model called `score_rank_relevel_fit` to predict average professor evaluation `score` based on `rank_relevel` of the professor.
    This is the new (releveled) variable you created in Exercise 12.
    Based on the regression output, write the linear model and interpret the slopes and intercept in context of the data.

```{r releveled-rank-score-regression}
score_rank_relevel_fit <- linear_reg() %>%
  set_engine("lm") %>%
  fit(score~rank_relevel, data = evals) %>%
  tidy()

score_rank_relevel_fit %>%
  flextable() %>% autofit()
```   

- *The linear model for this regression is:*

$$\widehat{score}_i = 4.15 + 0.13 \times teaching_i - 0.02 \times tenured_i$$

- *The intercept now reprecents the `tenure track` rank, with an average rating of 4.15, and the slopes represent the differences between that value and the other two, with the `teaching` rank's average being 0.13 points above the `tenure track` average and the `tenured` rank's average being 0.02 below it.*

14. Create another new variable called `tenure_eligible` that labels `"teaching"` faculty as `"no"` and labels `"tenure track"` and `"tenured"` faculty as `"yes"`.


üß∂ ‚úÖ ‚¨ÜÔ∏è Knit, *commit, and push your changes to GitHub with an appropriate commit message. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and review the md document on GitHub to make sure you're happy with the final state of your work.*
